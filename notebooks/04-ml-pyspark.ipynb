{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Machine Learning with PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Filter warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set font scale and style\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "\n",
    "# Normal distribution from scipy\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Pyspark modules\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import sql, SparkContext, SparkConf\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import Imputer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Pickle\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spark session\n",
    "spark = SparkSession.builder.appName(\"project\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: (5960, 13)\n",
      "+---+----+--------+--------+-------+------+----+-----+------+------------+----+----+------------+\n",
      "|bad|loan| mortdue|   value| reason|   job| yoj|derog|delinq|       clage|ninq|clno|     debtinc|\n",
      "+---+----+--------+--------+-------+------+----+-----+------+------------+----+----+------------+\n",
      "|  1|1100| 25860.0| 39025.0|HomeImp| Other|10.5|    0|     0|94.366666667|   1|   9|        null|\n",
      "|  1|1300| 70053.0| 68400.0|HomeImp| Other| 7.0|    0|     2|121.83333333|   0|  14|        null|\n",
      "|  1|1500| 13500.0| 16700.0|HomeImp| Other| 4.0|    0|     0|149.46666667|   1|  10|        null|\n",
      "|  1|1500|    null|    null|   null|  null|null| null|  null|        null|null|null|        null|\n",
      "|  0|1700| 97800.0|112000.0|HomeImp|Office| 3.0|    0|     0|93.333333333|   0|  14|        null|\n",
      "|  1|1700| 30548.0| 40320.0|HomeImp| Other| 9.0|    0|     0|101.46600191|   1|   8|37.113613558|\n",
      "|  1|1800| 48649.0| 57037.0|HomeImp| Other| 5.0|    3|     2|        77.1|   1|  17|        null|\n",
      "|  1|1800| 28502.0| 43034.0|HomeImp| Other|11.0|    0|     0|88.766029879|   0|   8|36.884894093|\n",
      "|  1|2000| 32700.0| 46740.0|HomeImp| Other| 3.0|    0|     2|216.93333333|   1|  12|        null|\n",
      "|  1|2000|    null| 62250.0|HomeImp| Sales|16.0|    0|     0|       115.8|   0|  13|        null|\n",
      "|  1|2000| 22608.0|    null|   null|  null|18.0| null|  null|        null|null|null|        null|\n",
      "|  1|2000| 20627.0| 29800.0|HomeImp|Office|11.0|    0|     1|122.53333333|   1|   9|        null|\n",
      "|  1|2000| 45000.0| 55000.0|HomeImp| Other| 3.0|    0|     0|86.066666667|   2|  25|        null|\n",
      "|  0|2000| 64536.0| 87400.0|   null|   Mgr| 2.5|    0|     0|147.13333333|   0|  24|        null|\n",
      "|  1|2100| 71000.0| 83850.0|HomeImp| Other| 8.0|    0|     1|       123.0|   0|  16|        null|\n",
      "|  1|2200| 24280.0| 34687.0|HomeImp| Other|null|    0|     1|300.86666667|   0|   8|        null|\n",
      "|  1|2200| 90957.0|102600.0|HomeImp|   Mgr| 7.0|    2|     6|       122.9|   1|  22|        null|\n",
      "|  1|2200| 23030.0|    null|   null|  null|19.0| null|  null|        null|null|null|3.7113122995|\n",
      "|  1|2300| 28192.0| 40150.0|HomeImp| Other| 4.5|    0|     0|        54.6|   1|  16|        null|\n",
      "|  0|2300|102370.0|120953.0|HomeImp|Office| 2.0|    0|     0|90.992533467|   0|  13|31.588503178|\n",
      "+---+----+--------+--------+-------+------+----+-----+------+------------+----+----+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('../data/hmeq.csv', inferSchema = True, header = True)  # load data\n",
    "\n",
    "df = df.toDF(*[c.lower() for c in df.columns]) # column names in lower case\n",
    "\n",
    "df.createOrReplaceTempView(\"raw_table\") # create table for sql query\n",
    "\n",
    "print(\"Data size:\", (df.count(), len(df.columns))) # print data size\n",
    "\n",
    "df.show() # display table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_numeric(df, cat_cols):\n",
    "    \"\"\"\n",
    "    Convert numerical columns in a spark dataframe to float\n",
    "    \n",
    "    Paramters    \n",
    "    ---------\n",
    "    df: spark dataframe        \n",
    "    cat_cols: list of categorical column names to be \n",
    "              removed before conversion\n",
    "    Returns        \n",
    "    --------        \n",
    "    spark dataframe with numerical columns as float  \n",
    "    \"\"\"\n",
    "    # Remove categorical column names        \n",
    "    col_names = [x for x in df.columns if x not in cat_cols]\n",
    "    for col_name in col_names:\n",
    "        df = df.withColumn(col_name, df[col_name].cast(FloatType()))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of categorical columns\n",
    "cat_cols = [field for (field, dataType) in df.dtypes if dataType == \"string\"]\n",
    "\n",
    "# Convert numerical cols to float\n",
    "df = df_to_numeric(df, cat_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data types and missing values\n",
    "\n",
    "The data shows that there are two categorical variables and some variables have missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- bad: integer (nullable = true)\n",
      " |-- loan: integer (nullable = true)\n",
      " |-- mortdue: double (nullable = true)\n",
      " |-- value: double (nullable = true)\n",
      " |-- reason: string (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- yoj: double (nullable = true)\n",
      " |-- derog: integer (nullable = true)\n",
      " |-- delinq: integer (nullable = true)\n",
      " |-- clage: double (nullable = true)\n",
      " |-- ninq: integer (nullable = true)\n",
      " |-- clno: integer (nullable = true)\n",
      " |-- debtinc: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-------+-----+------+----+----+-----+------+-----+----+----+-------+\n",
      "| bad|loan|mortdue|value|reason| job| yoj|derog|delinq|clage|ninq|clno|debtinc|\n",
      "+----+----+-------+-----+------+----+----+-----+------+-----+----+----+-------+\n",
      "|5960|5960|   5442| 5848|  5708|5681|5445| 5252|  5380| 5652|5450|5738|   4693|\n",
      "+----+----+-------+-----+------+----+----+-----+------+-----+----+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# count each column record\n",
    "df.agg(*[F.count(c).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------+-----+------+---+---+-----+------+-----+----+----+-------+\n",
      "|bad|loan|mortdue|value|reason|job|yoj|derog|delinq|clage|ninq|clno|debtinc|\n",
      "+---+----+-------+-----+------+---+---+-----+------+-----+----+----+-------+\n",
      "|  0|   0|    518|  112|   252|279|515|  708|   580|  308| 510| 222|   1267|\n",
      "+---+----+-------+-----+------+---+---+-----+------+-----+----+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# count each column missing values\n",
    "df.agg(*(F.sum(F.col(c).isNull().cast(\"int\"))\\\n",
    "            .alias(c) for c in df.columns)\n",
    "         ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impute missing categorical values\n",
    "All missing values in the categorical columns will be given another level called 'missing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of labels for missing vlaues in cat_cols\n",
    "dict_missing = {cat_col: 'missing' for cat_col in cat_cols}\n",
    "\n",
    "# Fill missing values in cat_cols\n",
    "df = df.fillna(dict_missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------------------+\n",
      "|bad|count|        percentage|\n",
      "+---+-----+------------------+\n",
      "|  1| 1189|19.949664429530202|\n",
      "|  0| 4771|  80.0503355704698|\n",
      "+---+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\" \n",
    "select \n",
    "    bad, \n",
    "    count(*) as count,\n",
    "    count(*)*100/(select count(*) from raw_table) as percentage\n",
    "from raw_table\n",
    "group by 1\n",
    "\"\"\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a test set\n",
    "We split the data into $80 \\%$ training set and $20 \\%$ test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF, testDF = df.randomSplit([.8, .2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (4817, 13)\n",
      "Test set size: (1143, 13)\n"
     ]
    }
   ],
   "source": [
    "print('Training set size:', (trainDF.count(), len(trainDF.columns)))\n",
    "print('Test set size:', (testDF.count(), len(testDF.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = [field for (field, dataType) in trainDF.dtypes if dataType == \"string\"]\n",
    "num_cols = [field for (field, dataType) in trainDF.dtypes if ((dataType == \"double\") \\\n",
    "                                                              & (field != \"bad\"))]\n",
    "\n",
    "index_output_cols = [x + \"index\" for x in cat_cols]\n",
    "ohe_output_cols = [x + \"ohe\" for x in cat_cols]\n",
    "num_imputed_output_cols = [x + \"imputed\" for x in num_cols]\n",
    "\n",
    "s_indexer = StringIndexer(inputCols = cat_cols, \n",
    "                          outputCols = index_output_cols, \n",
    "                          handleInvalid=\"skip\")\n",
    "\n",
    "cat_encoder = OneHotEncoder(inputCols = index_output_cols,\n",
    "                           outputCols = ohe_output_cols)\n",
    "\n",
    "num_imputer = Imputer(inputCols = num_cols, \n",
    "                      outputCols = num_imputed_output_cols)\n",
    "\n",
    "assembler_inputs = ohe_output_cols + num_imputed_output_cols\n",
    "\n",
    "assembler = VectorAssembler(inputCols = assembler_inputs, outputCol = \"unscaled_features\")\n",
    "scaler = StandardScaler(inputCol = 'unscaled_features', outputCol = \"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a logistic regression model.\n",
    "lr = LogisticRegression(labelCol = \"bad\", featuresCol = \"features\",\n",
    "                       maxIter=10, regParam=0.3, elasticNetParam=0.8, \n",
    "                        family=\"multinomial\")\n",
    "\n",
    "# Create Pipeline\n",
    "pipeline = Pipeline(stages = [s_indexer, cat_encoder, num_imputer, assembler, scaler, lr])\n",
    "\n",
    "# Fit the training set \n",
    "lr_model = pipeline.fit(trainDF)\n",
    "\n",
    "# prediction on the training set\n",
    "pred_lr = lr_model.transform(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+----------+\n",
      "|            features|bad|prediction|\n",
      "+--------------------+---+----------+\n",
      "|(6,[3],[2.6715770...|0.0|       0.0|\n",
      "|(6,[3],[2.6715770...|0.0|       0.0|\n",
      "|(6,[5],[5.5380902...|0.0|       0.0|\n",
      "|(6,[2],[2.3845409...|0.0|       0.0|\n",
      "|(6,[3],[2.6715770...|0.0|       0.0|\n",
      "|(6,[4],[2.9430833...|0.0|       0.0|\n",
      "|(6,[3],[2.6715770...|0.0|       0.0|\n",
      "|(6,[4],[2.9430833...|0.0|       0.0|\n",
      "|(6,[3],[2.6715770...|0.0|       0.0|\n",
      "|(6,[4],[2.9430833...|0.0|       0.0|\n",
      "+--------------------+---+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select example rows to display\n",
    "pred_lr.select(\"features\", \"bad\", \"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_trained_model = lr_model.stages[-1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric for Logistic Regression\n",
      "----------------------------------------\n",
      "Accuracy: 0.796\n",
      "Weighted Precision: 0.633\n",
      "Weighted Recall: 0.796\n",
      "F1-Score: 0.705\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator( \\\n",
    "                  labelCol='bad', \\\n",
    "                  predictionCol=\"prediction\", \\\n",
    "                  metricName=\"accuracy\")\n",
    "\n",
    "lr_accuracy = evaluator.evaluate(pred_lr)\n",
    "lr_prc = evaluator.evaluate(pred_lr, {evaluator.metricName: \"weightedPrecision\"})\n",
    "lr_recall = evaluator.evaluate(pred_lr, {evaluator.metricName: \"weightedRecall\"})\n",
    "lr_f1 = evaluator.evaluate(pred_lr, {evaluator.metricName: \"f1\"})\n",
    "\n",
    "print(\"Metric for Logistic Regression\")\n",
    "print('-'*40)\n",
    "print(\"Accuracy: %.3f\" % lr_accuracy)\n",
    "print(\"Weighted Precision: %.3f\" % lr_prc)\n",
    "print(\"Weighted Recall: %.3f\" % lr_recall)\n",
    "print(\"F1-Score: %.3f\" %lr_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "metadata": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
